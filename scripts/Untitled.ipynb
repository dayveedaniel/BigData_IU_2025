{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b089907-66c5-4a95-8d09-125e60432342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/18 23:31:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/18 23:31:07 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/05/18 23:31:07 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = \"team5\"\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "#We can also add\n",
    "# .config(\"spark.sql.catalogImplementation\",\"hive\")\\ \n",
    "# But this is the default configuration\n",
    "# You can switch to Spark Catalog by setting \"in-memory\" for \"spark.sql.catalogImplementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f43aef-a827-41f2-96bc-238b707f9623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             retake1|\n",
      "|             root_db|\n",
      "|                show|\n",
      "|     team0_projectdb|\n",
      "|    team11_projectdb|\n",
      "|           team12_db|\n",
      "|team12_hive_proje...|\n",
      "|    team12_projectdb|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team17_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "|    team21_projectdb|\n",
      "| team21_projectdb_v2|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+---------------+--------------+-----------+\n",
      "|      namespace|     tableName|isTemporary|\n",
      "+---------------+--------------+-----------+\n",
      "|team5_projectdb|accidents_part|      false|\n",
      "|team5_projectdb|locations_part|      false|\n",
      "|team5_projectdb|    q1_results|      false|\n",
      "|team5_projectdb|    q2_results|      false|\n",
      "|team5_projectdb|    q3_results|      false|\n",
      "|team5_projectdb|    q4_results|      false|\n",
      "|team5_projectdb|    q5_results|      false|\n",
      "|team5_projectdb|    q6_results|      false|\n",
      "|team5_projectdb|    q7_results|      false|\n",
      "|team5_projectdb| road_features|      false|\n",
      "|team5_projectdb|      twilight|      false|\n",
      "|team5_projectdb|  weather_buck|      false|\n",
      "+---------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "spark.sql(\"USE team5_projectdb\").show()\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "# spark.sql(\"SELECT * FROM <db_name>.<table_name>\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973def47-0de0-4805-be64-1da65ae999e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accidents_part\n",
      "+------------+\n",
      "|    col_name|\n",
      "+------------+\n",
      "|          id|\n",
      "|      source|\n",
      "| distance_mi|\n",
      "| description|\n",
      "| location_id|\n",
      "|  weather_id|\n",
      "| twilight_id|\n",
      "|road_feat_id|\n",
      "|  start_time|\n",
      "|    end_time|\n",
      "|       month|\n",
      "|    severity|\n",
      "|        year|\n",
      "+------------+\n",
      "\n",
      "locations_part\n",
      "+---------+\n",
      "| col_name|\n",
      "+---------+\n",
      "|       id|\n",
      "|start_lat|\n",
      "|start_lng|\n",
      "|  end_lat|\n",
      "|  end_lng|\n",
      "|   street|\n",
      "|     city|\n",
      "|   county|\n",
      "|  zipcode|\n",
      "| timezone|\n",
      "|    state|\n",
      "+---------+\n",
      "\n",
      "road_features\n",
      "+---------------+\n",
      "|       col_name|\n",
      "+---------------+\n",
      "|             id|\n",
      "|        amenity|\n",
      "|           bump|\n",
      "|       crossing|\n",
      "|       give_way|\n",
      "|       junction|\n",
      "|        no_exit|\n",
      "|        railway|\n",
      "|     roundabout|\n",
      "|        station|\n",
      "|           stop|\n",
      "|traffic_calming|\n",
      "| traffic_signal|\n",
      "+---------------+\n",
      "\n",
      "twilight\n",
      "+--------------------+\n",
      "|            col_name|\n",
      "+--------------------+\n",
      "|                  id|\n",
      "|      sunrise_sunset|\n",
      "|      civil_twilight|\n",
      "|   nautical_twilight|\n",
      "|astronomical_twil...|\n",
      "+--------------------+\n",
      "\n",
      "weather_buck\n",
      "+-----------------+\n",
      "|         col_name|\n",
      "+-----------------+\n",
      "|               id|\n",
      "|     airport_code|\n",
      "|weather_timestamp|\n",
      "|    temperature_f|\n",
      "|     wind_chill_f|\n",
      "|     humidity_pct|\n",
      "|      pressure_in|\n",
      "|    visibility_mi|\n",
      "|   wind_direction|\n",
      "|   wind_speed_mph|\n",
      "| precipitation_in|\n",
      "|weather_condition|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = [row['tableName'] for row in spark.sql(\"SHOW TABLES\").select('tableName').collect()]\n",
    "tables = [table for table in tables if \"_results\" not in table]\n",
    "\n",
    "for table in tables:\n",
    "    print(table)\n",
    "    spark.sql(f\"SHOW COLUMNS IN team5_projectdb.{table}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad65a71-3806-4213-8d37-5ba25dc1f891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accidents_part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 23:31:31 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----------+--------------------+-----------+----------+-----------+------------+-------------------+-------------------+-----+--------+----+\n",
      "|       id| source|distance_mi|         description|location_id|weather_id|twilight_id|road_feat_id|         start_time|           end_time|month|severity|year|\n",
      "+---------+-------+-----------+--------------------+-----------+----------+-----------+------------+-------------------+-------------------+-----+--------+----+\n",
      "|A-6710055|Source1|       1.01|Closed at Warrens...|    3764532|   3162384|          5|         258|2020-07-21 23:17:37|2020-07-21 23:47:37|    7|       4|2020|\n",
      "+---------+-------+-----------+--------------------+-----------+----------+-----------+------------+-------------------+-------------------+-----+--------+----+\n",
      "only showing top 1 row\n",
      "\n",
      "locations_part\n",
      "+-------+---------+---------+-------+-------+------+-------+------+----------+--------+-----+\n",
      "|     id|start_lat|start_lng|end_lat|end_lng|street|   city|county|   zipcode|timezone|state|\n",
      "+-------+---------+---------+-------+-------+------+-------+------+----------+--------+-----+\n",
      "|3803932|       43|     -123|   NULL|   NULL| I-5 S|Douglas| 97471|US/Pacific|Roseburg|   OR|\n",
      "+-------+---------+---------+-------+-------+------+-------+------+----------+--------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "road_features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+\n",
      "| id|amenity| bump|crossing|give_way|junction|no_exit|railway|roundabout|station| stop|traffic_calming|traffic_signal|\n",
      "+---+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+\n",
      "|  1|   true|false|    true|    true|    true|  false|  false|     false|   true|false|          false|          true|\n",
      "+---+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "twilight\n",
      "+---+--------------+--------------+-----------------+---------------------+\n",
      "| id|sunrise_sunset|civil_twilight|nautical_twilight|astronomical_twilight|\n",
      "+---+--------------+--------------+-----------------+---------------------+\n",
      "|  1|         Night|           Day|              Day|                Night|\n",
      "+---+--------------+--------------+-----------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "weather_buck\n",
      "+-------+------------+-------------------+-------------+------------+------------+-----------+-------------+--------------+--------------+----------------+-----------------+\n",
      "|     id|airport_code|  weather_timestamp|temperature_f|wind_chill_f|humidity_pct|pressure_in|visibility_mi|wind_direction|wind_speed_mph|precipitation_in|weather_condition|\n",
      "+-------+------------+-------------------+-------------+------------+------------+-----------+-------------+--------------+--------------+----------------+-----------------+\n",
      "|4369353|        KDYB|2020-05-26 17:15:00|         NULL|        NULL|        NULL|       NULL|         NULL|          NULL|          NULL|               0|             NULL|\n",
      "+-------+------------+-------------------+-------------+------------+------------+-----------+-------------+--------------+--------------+----------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for table in tables:\n",
    "    print(table)\n",
    "    spark.sql(f\"SELECT * FROM team5_projectdb.{table}\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b39fae-1318-4bd6-9886-de363753eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for table in tables:\n",
    "    data[table] = spark.read.format(\"avro\").table(f\"team5_projectdb.{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2ba1b6d-5a10-4fdc-ade3-7bb097e2edd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['accidents_part', 'locations_part', 'road_features', 'twilight', 'weather_buck'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33742c0e-6c77-41fe-aee9-c3e96a344439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:====================================================> (662 + 2) / 684]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+------------------+------------------+\n",
      "|start_lat_missing|start_lng_missing|   end_lat_missing|   end_lng_missing|\n",
      "+-----------------+-----------------+------------------+------------------+\n",
      "|              0.0|              0.0|0.3335100786915551|0.3335100786915551|\n",
      "+-----------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnull\n",
    "\n",
    "# Calculate missing value percentages\n",
    "total_count = data['locations_part'].count()\n",
    "\n",
    "missing_stats = data['locations_part'].agg(\n",
    "    (count(when(isnull(col(\"start_lat\")), col(\"id\"))) / total_count).alias(\"start_lat_missing\"),\n",
    "    (count(when(isnull(col(\"start_lng\")), col(\"id\"))) / total_count).alias(\"start_lng_missing\"),\n",
    "    (count(when(isnull(col(\"end_lat\")), col(\"id\"))) / total_count).alias(\"end_lat_missing\"),\n",
    "    (count(when(isnull(col(\"end_lng\")), col(\"id\"))) / total_count).alias(\"end_lng_missing\")\n",
    ")\n",
    "\n",
    "missing_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf550b0c-158c-4cf0-8576-0e452dc63bc2",
   "metadata": {},
   "source": [
    "From the table we can see that we always have starting coordinates of the accident and about 33.35% of the times we don't have ending coordinates. We will impute missing coordinates with starting coordinates, as it's most likely that those accidents are single point, i.e., accident did not involve movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef7ec080-87de-46d8-bcbb-ec7bb6729edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, col\n",
    "\n",
    "# Impute missing end locations with start locations\n",
    "data['locations_part'] = data['locations_part'].withColumn(\n",
    "    \"end_lat\", coalesce(col(\"end_lat\"), col(\"start_lat\")))\n",
    "    \n",
    "data['locations_part'] = data['locations_part'].withColumn(\n",
    "    \"end_lng\", coalesce(col(\"end_lng\"), col(\"start_lng\")))\n",
    "\n",
    "# Add flag indicating imputation\n",
    "data['locations_part'] = data['locations_part'].withColumn(\n",
    "    \"end_loc_imputed\", \n",
    "    (col(\"end_lat\") == col(\"start_lat\")) & (col(\"end_lng\") == col(\"start_lng\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd38d5a2-84b8-452a-8503-45cd78455337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+-------+-------+------------------+----+----------+----------+--------+-----+---------------+\n",
      "|     id|start_lat|start_lng|end_lat|end_lng|            street|city|    county|   zipcode|timezone|state|end_loc_imputed|\n",
      "+-------+---------+---------+-------+-------+------------------+----+----------+----------+--------+-----+---------------+\n",
      "|3797705|       39|      -76|     39|    -76|     Canterbury Rd|Kent|     19943|US/Eastern|  Felton|   DE|           true|\n",
      "|3791535|       39|      -76|     39|    -76|      S Dupont Hwy|Kent|     19943|US/Eastern|  Felton|   DE|           true|\n",
      "|3788821|       39|      -76|     39|    -76|      S Dupont Hwy|Kent|     19901|US/Eastern|   Dover|   DE|           true|\n",
      "|3786284|       39|      -76|     39|    -76| Wheatleys Pond Rd|Kent|19977-3799|US/Eastern|  Smyrna|   DE|           true|\n",
      "|3775460|       39|      -76|     39|    -76|      S Dupont Hwy|Kent|     19901|US/Eastern|   Dover|   DE|           true|\n",
      "+-------+---------+---------+-------+-------+------------------+----+----------+----------+--------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['locations_part'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9090a424-5b05-48ef-926f-e4ae57c59302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, unix_timestamp, col\n",
    "\n",
    "# Define the desired format (optional for from_unixtime, default is 'yyyy-MM-dd HH:mm:ss')\n",
    "datetime_format = \"yyyy-MM-dd HH:mm:ss\"\n",
    "\n",
    "data['accidents_part'] = data['accidents_part'].withColumn(\n",
    "    \"start_datetime\",\n",
    "    from_unixtime(unix_timestamp(col(\"start_time\")), datetime_format)\n",
    ").withColumn(\n",
    "    \"end_datetime\",\n",
    "    from_unixtime(unix_timestamp(col(\"end_time\")), datetime_format)\n",
    ")\n",
    "\n",
    "data['weather_buck'] = data['weather_buck'].withColumn(\n",
    "    \"weather_datetime\",\n",
    "    from_unixtime(unix_timestamp(col(\"weather_timestamp\")), datetime_format)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67ad4543-b45e-47da-abd2-cb25430b23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second\n",
    "\n",
    "# Extract components for accidents\n",
    "data['accidents_part'] = data['accidents_part'].withColumn(\"start_year\", year(\"start_datetime\")) \\\n",
    "    .withColumn(\"start_month\", month(\"start_datetime\")) \\\n",
    "    .withColumn(\"start_day\", dayofmonth(\"start_datetime\")) \\\n",
    "    .withColumn(\"start_hour\", hour(\"start_datetime\")) \\\n",
    "    .withColumn(\"start_minute\", minute(\"start_datetime\")) \\\n",
    "    .withColumn(\"start_second\", second(\"start_datetime\"))\n",
    "data['accidents_part'] = data['accidents_part'].withColumn(\"end_year\", year(\"end_datetime\")) \\\n",
    "    .withColumn(\"end_month\", month(\"end_datetime\")) \\\n",
    "    .withColumn(\"end_day\", dayofmonth(\"end_datetime\")) \\\n",
    "    .withColumn(\"end_hour\", hour(\"end_datetime\")) \\\n",
    "    .withColumn(\"end_minute\", minute(\"end_datetime\")) \\\n",
    "    .withColumn(\"end_second\", second(\"end_datetime\"))\n",
    "\n",
    "# Extract components for weather\n",
    "data['weather_buck'] = data['weather_buck'].withColumn(\"weather_year\", year(\"weather_datetime\")) \\\n",
    "    .withColumn(\"weather_month\", month(\"weather_datetime\")) \\\n",
    "    .withColumn(\"weather_day\", dayofmonth(\"weather_datetime\")) \\\n",
    "    .withColumn(\"weather_hour\", hour(\"weather_datetime\")) \\\n",
    "    .withColumn(\"weather_minute\", minute(\"weather_datetime\")) \\\n",
    "    .withColumn(\"weather_second\", second(\"weather_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e18f64a0-a855-4d1d-9709-d51eea846543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, datediff, unix_timestamp, date_format, when, dayofweek\n",
    "\n",
    "# Duration of accident in minutes\n",
    "data['accidents_part'] = data['accidents_part'].withColumn(\n",
    "    \"duration_minutes\",\n",
    "    (unix_timestamp(col(\"end_time\")) - unix_timestamp(col(\"start_time\"))) / 60.0\n",
    ")\n",
    "\n",
    "# Day of week (1=Sunday, 7=Saturday)\n",
    "data['accidents_part'] = data['accidents_part'].withColumn(\n",
    "    \"day_of_week\",\n",
    "    dayofweek(col(\"start_datetime\"))  # Returns 1 (Sunday) to 7 (Saturday)\n",
    ")\n",
    "\n",
    "# Weekend flag (Saturday=7, Sunday=1)\n",
    "data['accidents_part'] = data['accidents_part'].withColumn(\n",
    "    \"is_weekend\",\n",
    "    ((col(\"day_of_week\") == 1) | (col(\"day_of_week\") == 7)).cast(\"integer\")\n",
    ")\n",
    "\n",
    "\n",
    "# Season (could be useful for weather patterns)\n",
    "data['accidents_part'] = data['accidents_part'].withColumn(\n",
    "    \"season\",\n",
    "    when((col(\"start_month\") >= 3) & (col(\"start_month\") <= 5), \"spring\")\n",
    "    .when((col(\"start_month\") >= 6) & (col(\"start_month\") <= 8), \"summer\")\n",
    "    .when((col(\"start_month\") >= 9) & (col(\"start_month\") <= 11), \"fall\")\n",
    "    .otherwise(\"winter\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a852f2-0b30-4abe-baba-d698eeedb4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accidents_part': DataFrame[id: string, source: string, distance_mi: decimal(10,2), description: string, location_id: bigint, weather_id: bigint, twilight_id: int, road_feat_id: int, start_time: timestamp, end_time: timestamp, month: int, severity: smallint, year: int, start_datetime: string, end_datetime: string, start_year: int, start_month: int, start_day: int, start_hour: int, start_minute: int, start_second: int, end_year: int, end_month: int, end_day: int, end_hour: int, end_minute: int, end_second: int, duration_minutes: double, day_of_week: int, is_weekend: int, season: string],\n",
       " 'locations_part': DataFrame[id: bigint, start_lat: decimal(10,0), start_lng: decimal(10,0), end_lat: decimal(10,0), end_lng: decimal(10,0), street: string, city: string, county: string, zipcode: string, timezone: string, state: string, end_loc_imputed: boolean],\n",
       " 'road_features': DataFrame[id: int, amenity: boolean, bump: boolean, crossing: boolean, give_way: boolean, junction: boolean, no_exit: boolean, railway: boolean, roundabout: boolean, station: boolean, stop: boolean, traffic_calming: boolean, traffic_signal: boolean],\n",
       " 'twilight': DataFrame[id: int, sunrise_sunset: string, civil_twilight: string, nautical_twilight: string, astronomical_twilight: string],\n",
       " 'weather_buck': DataFrame[id: bigint, airport_code: string, weather_timestamp: timestamp, temperature_f: decimal(10,0), wind_chill_f: decimal(10,0), humidity_pct: decimal(10,0), pressure_in: decimal(10,0), visibility_mi: decimal(10,0), wind_direction: string, wind_speed_mph: decimal(10,0), precipitation_in: decimal(10,0), weather_condition: string, weather_datetime: string, weather_year: int, weather_month: int, weather_day: int, weather_hour: int, weather_minute: int, weather_second: int]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b78a4d5-16b4-4603-8e4c-cd4300fae47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Join all tables\n",
    "df = data['accidents_part'] \\\n",
    "    .join(data['locations_part'], data['accidents_part'].location_id == data['locations_part'].id, 'left') \\\n",
    "    .join(data['weather_buck'], data['accidents_part'].weather_id == data['weather_buck'].id, 'left') \\\n",
    "    .join(data['twilight'], data['accidents_part'].twilight_id == data['twilight'].id, 'left') \\\n",
    "    .join(data['road_features'], data['accidents_part'].road_feat_id == data['road_features'].id, 'left')\n",
    "\n",
    "# Drop redundant ID columns\n",
    "df = df.drop('location_id', 'weather_id', 'twilight_id', 'road_feat_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06ebe3e7-03d0-45aa-86b5-8d21198e108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id       |description                                                                                                                                              |\n",
      "+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|A-5429427|Slow traffic on Southern State Pkwy E from Eagle Ave/Exit 18 (Southern State Pkwy) to Baldwin Rd/Grand Ave/Exit 20 (Southern State Pkwy) due to accident.|\n",
      "|A-4575117|Accident from exit [284A] to I-30 E.                                                                                                                     |\n",
      "|A-3936576|Incident on HIGH HOUSE RD near DAVIS DR Expect delays.                                                                                                   |\n",
      "|A-3983725|Incident on I-64 EB near MM 152 Expect delays.                                                                                                           |\n",
      "|A-3963444|Accident on I-80 Bus W from E St (I-80 Bus) to US-50/CA-99 (I-80 Bus).                                                                                   |\n",
      "|A-3923327|Incident on I-5 NB near LAKE ST Expect delays.                                                                                                           |\n",
      "|A-3959500|Incident on I-64 EB near I-64 Drive with caution.                                                                                                        |\n",
      "|A-4155805|Incident on GUADALUPE ST near W ST JOHNS AVE Drive with caution.                                                                                         |\n",
      "|A-4780570|Incident on I-95 SB near MM 160 Drive with caution.                                                                                                      |\n",
      "|A-4838404|Incident on I-270 near GRAHAM RD Expect delays.                                                                                                          |\n",
      "+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['accidents_part'].select('id', 'description').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e872458-bc00-4441-a294-4e73e276a485",
   "metadata": {},
   "source": [
    "As we can see, description is mainly giving us information about the location of the accidents - we already have that, so that information is not really useful and we can safely drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "325ad38f-ade1-43d6-998b-cc37c4c2dec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns after joining:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- distance_mi: decimal(10,2) (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- severity: short (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- start_year: integer (nullable = true)\n",
      " |-- start_month: integer (nullable = true)\n",
      " |-- start_day: integer (nullable = true)\n",
      " |-- start_hour: integer (nullable = true)\n",
      " |-- start_minute: integer (nullable = true)\n",
      " |-- start_second: integer (nullable = true)\n",
      " |-- end_year: integer (nullable = true)\n",
      " |-- end_month: integer (nullable = true)\n",
      " |-- end_day: integer (nullable = true)\n",
      " |-- end_hour: integer (nullable = true)\n",
      " |-- end_minute: integer (nullable = true)\n",
      " |-- end_second: integer (nullable = true)\n",
      " |-- duration_minutes: double (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- is_weekend: integer (nullable = true)\n",
      " |-- season: string (nullable = false)\n",
      " |-- start_lat: decimal(10,0) (nullable = true)\n",
      " |-- start_lng: decimal(10,0) (nullable = true)\n",
      " |-- end_lat: decimal(10,0) (nullable = true)\n",
      " |-- end_lng: decimal(10,0) (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zipcode: string (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- airport_code: string (nullable = true)\n",
      " |-- temperature_f: decimal(10,0) (nullable = true)\n",
      " |-- wind_chill_f: decimal(10,0) (nullable = true)\n",
      " |-- humidity_pct: decimal(10,0) (nullable = true)\n",
      " |-- pressure_in: decimal(10,0) (nullable = true)\n",
      " |-- visibility_mi: decimal(10,0) (nullable = true)\n",
      " |-- wind_direction: string (nullable = true)\n",
      " |-- wind_speed_mph: decimal(10,0) (nullable = true)\n",
      " |-- precipitation_in: decimal(10,0) (nullable = true)\n",
      " |-- weather_condition: string (nullable = true)\n",
      " |-- weather_year: integer (nullable = true)\n",
      " |-- weather_month: integer (nullable = true)\n",
      " |-- weather_day: integer (nullable = true)\n",
      " |-- weather_hour: integer (nullable = true)\n",
      " |-- weather_minute: integer (nullable = true)\n",
      " |-- weather_second: integer (nullable = true)\n",
      " |-- sunrise_sunset: string (nullable = true)\n",
      " |-- civil_twilight: string (nullable = true)\n",
      " |-- nautical_twilight: string (nullable = true)\n",
      " |-- astronomical_twilight: string (nullable = true)\n",
      " |-- amenity: boolean (nullable = true)\n",
      " |-- bump: boolean (nullable = true)\n",
      " |-- crossing: boolean (nullable = true)\n",
      " |-- give_way: boolean (nullable = true)\n",
      " |-- junction: boolean (nullable = true)\n",
      " |-- no_exit: boolean (nullable = true)\n",
      " |-- railway: boolean (nullable = true)\n",
      " |-- roundabout: boolean (nullable = true)\n",
      " |-- station: boolean (nullable = true)\n",
      " |-- stop: boolean (nullable = true)\n",
      " |-- traffic_calming: boolean (nullable = true)\n",
      " |-- traffic_signal: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# First, let's select only the needed columns from each table before joining\n",
    "accidents_selected = data['accidents_part'].select(\n",
    "    [c for c in data['accidents_part'].columns]\n",
    ")\n",
    "\n",
    "locations_selected = data['locations_part'].select(\n",
    "    'id', 'start_lat', 'start_lng', 'end_lat', 'end_lng',\n",
    "    'street', 'city', 'county', 'state', 'zipcode', \n",
    "    'timezone'\n",
    ")\n",
    "\n",
    "weather_selected = data['weather_buck'].select(\n",
    "    'id', 'airport_code', 'temperature_f', 'wind_chill_f', 'humidity_pct', \n",
    "    'pressure_in', 'visibility_mi', 'wind_direction', 'wind_speed_mph', \n",
    "    'precipitation_in', 'weather_condition', 'weather_year', 'weather_month', \n",
    "    'weather_day', 'weather_hour', 'weather_minute', 'weather_second'\n",
    ")\n",
    "\n",
    "twilight_selected = data['twilight'].select(\n",
    "    [c for c in data['twilight'].columns]\n",
    ")\n",
    "\n",
    "road_features_selected = data['road_features'].select(\n",
    "    [c for c in data['road_features'].columns]\n",
    ")\n",
    "\n",
    "# Now perform the join with explicit column selection\n",
    "df = accidents_selected \\\n",
    "    .join(locations_selected, accidents_selected.location_id == locations_selected.id, 'left') \\\n",
    "    .join(weather_selected, accidents_selected.weather_id == weather_selected.id, 'left') \\\n",
    "    .join(twilight_selected, accidents_selected.twilight_id == twilight_selected.id, 'left') \\\n",
    "    .join(road_features_selected, accidents_selected.road_feat_id == road_features_selected.id, 'left')\n",
    "\n",
    "# Drop the redundant ID columns from joined tables\n",
    "df = df.drop(\n",
    "    locations_selected.id, \n",
    "    weather_selected.id, \n",
    "    twilight_selected.id, \n",
    "    road_features_selected.id,\n",
    "    'location_id', \n",
    "    'weather_id', \n",
    "    'twilight_id', \n",
    "    'road_feat_id'\n",
    ")\n",
    "\n",
    "# Also drop any other redundant columns that might have been carried over\n",
    "redundant_cols = [\n",
    "    'start_datetime', 'end_datetime',  # We have the decomposed time features\n",
    "    'weather_datetime',                # Same as above\n",
    "    'description'                      # Text field not useful for ML\n",
    "]\n",
    "\n",
    "df = df.drop(*[c for c in redundant_cols if c in df.columns])\n",
    "\n",
    "# Verify the remaining columns\n",
    "print(\"Final columns after joining:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "634a0620-8e92-4721-a48d-51c6e0c0146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "import math\n",
    "\n",
    "class CyclicalTimeTransformer(Transformer, HasInputCol, HasOutputCol, \n",
    "                            DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \"\"\"\n",
    "    A custom transformer that converts cyclical time features (like hour, month, etc.)\n",
    "    into sin/cos components to preserve their cyclical nature.\n",
    "    \"\"\"\n",
    "    \n",
    "    period = Param(Params._dummy(), \"period\", \"The period of the cyclical feature (e.g., 12 for months, 24 for hours)\",\n",
    "                  typeConverter=TypeConverters.toFloat)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, period=None):\n",
    "        \"\"\"\n",
    "        Initialize the transformer.\n",
    "        \n",
    "        :param inputCol: The name of the input column (time component to transform)\n",
    "        :param outputCol: The base name for output columns (will append _sin and _cos)\n",
    "        :param period: The period of the cyclical feature (e.g., 12 for months)\n",
    "        \"\"\"\n",
    "        super(CyclicalTimeTransformer, self).__init__()\n",
    "        self._setDefault(period=12.0)  # Default to monthly cycle\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "    \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, period=None):\n",
    "        \"\"\"\n",
    "        Set the params for this CyclicalTimeTransformer.\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getPeriod(self):\n",
    "        \"\"\"\n",
    "        Gets the value of period or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.period)\n",
    "    \n",
    "    def _transform(self, dataset: DataFrame):\n",
    "        \"\"\"\n",
    "        Transform the input dataset by adding sin/cos components of the cyclical feature.\n",
    "        \"\"\"\n",
    "        input_col = self.getInputCol()\n",
    "        output_col = self.getOutputCol()\n",
    "        period = self.getPeriod()\n",
    "        \n",
    "        # Calculate the sin and cos components\n",
    "        angle = 2 * math.pi * F.col(input_col) / period\n",
    "        \n",
    "        return dataset.withColumn(f\"{output_col}_sin\", F.sin(angle)) \\\n",
    "                     .withColumn(f\"{output_col}_cos\", F.cos(angle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccb0b82a-2752-4d0b-90d7-f72430989901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['start', 'end', 'weather']\n",
    "cyclical_time_transformers = []\n",
    "for col in cols:\n",
    "    # Transform months (period=12)\n",
    "    cyclical_time_transformers.append(\n",
    "        CyclicalTimeTransformer(\n",
    "            inputCol=f\"{col}_month\", \n",
    "            outputCol=f\"{col}_month_encoded\",\n",
    "            period=12.0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Transform days (period=31)\n",
    "    cyclical_time_transformers.append(\n",
    "        CyclicalTimeTransformer(\n",
    "            inputCol=f\"{col}_day\", \n",
    "            outputCol=f\"{col}_day_encoded\",\n",
    "            period=31.0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Transform hours (period=24)\n",
    "    cyclical_time_transformers.append(\n",
    "        CyclicalTimeTransformer(\n",
    "            inputCol=f\"{col}_hour\",\n",
    "            outputCol=f\"{col}_hour_encoded\",\n",
    "            period=24.0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Transform minutes (period=60)\n",
    "    cyclical_time_transformers.append(\n",
    "        CyclicalTimeTransformer(\n",
    "            inputCol=f\"{col}_minute\",\n",
    "            outputCol=f\"{col}_minute_encoded\",\n",
    "            period=60.0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Transform seconds (period=60)\n",
    "    cyclical_time_transformers.append(\n",
    "        CyclicalTimeTransformer(\n",
    "            inputCol=f\"{col}_second\",\n",
    "            outputCol=f\"{col}_second_encoded\",\n",
    "            period=60.0\n",
    "        )\n",
    "    )\n",
    "len(cyclical_time_transformers) # Should be 3*5=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42af4380-ad9d-4940-81f4-7416954c5491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCols, HasOutputCols\n",
    "import pyspark.sql.functions as F\n",
    "import math\n",
    "\n",
    "class GeodeticToECEFTransformer(Transformer, HasInputCols, HasOutputCols):\n",
    "    \"\"\"\n",
    "    Converts geodetic coordinates (lat, lng, alt) to ECEF coordinates (x, y, z)\n",
    "    WGS84 ellipsoid parameters used by default\n",
    "    \"\"\"\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, outputCols=None):\n",
    "        super(GeodeticToECEFTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "    \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, outputCols=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        # WGS84 parameters\n",
    "        a = 6378137.0  # semi-major axis in meters\n",
    "        f = 1/298.257223563  # flattening\n",
    "        b = a * (1 - f)  # semi-minor axis\n",
    "        \n",
    "        lat_col, lng_col, alt_col = self.getInputCols()\n",
    "        x_col, y_col, z_col = self.getOutputCols()\n",
    "        \n",
    "        # Convert to radians\n",
    "        lat_rad = F.radians(F.col(lat_col))\n",
    "        lng_rad = F.radians(F.col(lng_col))\n",
    "        \n",
    "        # Compute eccentricity\n",
    "        e_sq = 2*f - f*f\n",
    "        \n",
    "        # Compute N (prime vertical radius of curvature)\n",
    "        N = a / F.sqrt(1 - e_sq * F.sin(lat_rad)**2)\n",
    "        \n",
    "        # Compute ECEF coordinates\n",
    "        x = (N + F.col(alt_col)) * F.cos(lat_rad) * F.cos(lng_rad)\n",
    "        y = (N + F.col(alt_col)) * F.cos(lat_rad) * F.sin(lng_rad)\n",
    "        z = ((1 - e_sq) * N + F.col(alt_col)) * F.sin(lat_rad)\n",
    "        \n",
    "        return dataset.withColumn(x_col, x) \\\n",
    "                     .withColumn(y_col, y) \\\n",
    "                     .withColumn(z_col, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfd55eee-4b72-407c-a321-31ea8aafe07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geodetic_transformers = []\n",
    "for pref in ['start', 'end']:\n",
    "    if f'{pref}_alt' not in df.columns:\n",
    "        df = df.withColumn(f'{pref}_alt', F.lit(0.0))\n",
    "    geodetic_transformers.append(\n",
    "        GeodeticToECEFTransformer(\n",
    "            inputCols=[f'{pref}_lat', f'{pref}_lng', f'{pref}_alt'],\n",
    "            outputCols=[f'{pref}_x', f'{pref}_y', f'{pref}_z']\n",
    "        )\n",
    "    )\n",
    "\n",
    "len(geodetic_transformers) # Should be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "962dcbfb-3e67-4406-aee6-1877a8d6316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, FeatureHasher\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# df = df.withColumn(\"address\", concat_ws(\", \", \"street\", \"city\", \"county\", \"state\", \"zipcode\"))\n",
    "\n",
    "address_hasher = FeatureHasher(\n",
    "    inputCols=[\"street\", \"city\", \"county\", \"state\", \"zipcode\"],\n",
    "    outputCol=\"address_hashed\",\n",
    "    numFeatures=64  # Reduced from default 2^18 to save memory\n",
    ")\n",
    "zipcode_indexer = StringIndexer(inputCol=\"zipcode\", outputCol=\"zipcode_encoded\", handleInvalid=\"keep\")\n",
    "timezone_indexer = StringIndexer(inputCol=\"timezone\", outputCol=\"timezone_encoded\", handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d21379c4-f753-4c29-ad60-9c8dcc333dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for transformer in cyclical_time_transformers+geodetic_transformers:\n",
    "    df = transformer.transform(df)\n",
    "df = address_hasher.transform(df)\n",
    "df = zipcode_indexer.fit(df).transform(df)\n",
    "df = timezone_indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fd5689e-32df-4886-afbb-85d0afb55f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 23:33:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id='A-3464302', source='Source1', distance_mi=Decimal('0.50'), start_time=datetime.datetime(2016, 10, 25, 12, 15, 41), end_time=datetime.datetime(2016, 10, 25, 18, 15, 41), month=10, severity=3, year=2016, start_year=2016, start_month=10, start_day=25, start_hour=12, start_minute=15, start_second=41, end_year=2016, end_month=10, end_day=25, end_hour=18, end_minute=15, end_second=41, duration_minutes=360.0, day_of_week=3, is_weekend=0, season='fall', start_lat=Decimal('40'), start_lng=Decimal('-86'), end_lat=Decimal('40'), end_lng=Decimal('-86'), street=' N Dearborn St', city='Marion', county='46218-3744', state='IN', zipcode='US/Eastern', timezone='Indianapolis', airport_code='KEYE', temperature_f=Decimal('42'), wind_chill_f=Decimal('39'), humidity_pct=Decimal('82'), pressure_in=Decimal('30'), visibility_mi=Decimal('10'), wind_direction='East', wind_speed_mph=Decimal('5'), precipitation_in=None, weather_condition='Clear', weather_year=2016, weather_month=10, weather_day=25, weather_hour=8, weather_minute=53, weather_second=0, sunrise_sunset='Day', civil_twilight='Day', nautical_twilight='Day', astronomical_twilight='Day', amenity=False, bump=False, crossing=True, give_way=False, junction=False, no_exit=False, railway=False, roundabout=False, station=False, stop=False, traffic_calming=False, traffic_signal=False, start_alt=0.0, end_alt=0.0, start_month_encoded_sin=-0.8660254037844386, start_month_encoded_cos=0.5000000000000001, start_day_encoded_sin=-0.9377521321470804, start_day_encoded_cos=0.3473052528448203, start_hour_encoded_sin=1.2246467991473532e-16, start_hour_encoded_cos=-1.0, start_minute_encoded_sin=1.0, start_minute_encoded_cos=2.83276944882399e-16, start_second_encoded_sin=-0.9135454576426005, start_second_encoded_cos=-0.40673664307580093, end_month_encoded_sin=-0.8660254037844386, end_month_encoded_cos=0.5000000000000001, end_day_encoded_sin=-0.9377521321470804, end_day_encoded_cos=0.3473052528448203, end_hour_encoded_sin=-1.0, end_hour_encoded_cos=-1.8369701987210297e-16, end_minute_encoded_sin=1.0, end_minute_encoded_cos=2.83276944882399e-16, end_second_encoded_sin=-0.9135454576426005, end_second_encoded_cos=-0.40673664307580093, weather_month_encoded_sin=-0.8660254037844386, weather_month_encoded_cos=0.5000000000000001, weather_day_encoded_sin=-0.9377521321470804, weather_day_encoded_cos=0.3473052528448203, weather_hour_encoded_sin=0.8660254037844387, weather_hour_encoded_cos=-0.4999999999999998, weather_minute_encoded_sin=-0.6691306063588581, weather_minute_encoded_cos=0.7431448254773942, weather_second_encoded_sin=0.0, weather_second_encoded_cos=1.0, start_x=341298.02924215276, start_y=-4880789.210265539, start_z=4077985.572200376, end_x=341298.02924215276, end_y=-4880789.210265539, end_z=4077985.572200376, address_hashed=SparseVector(64, {5: 1.0, 26: 1.0, 43: 1.0, 45: 1.0, 58: 1.0}), zipcode_encoded=0.0, timezone_encoded=44.0),\n",
       " Row(id='A-3464290', source='Source1', distance_mi=Decimal('0.00'), start_time=datetime.datetime(2016, 10, 25, 12, 4, 29), end_time=datetime.datetime(2016, 10, 25, 18, 4, 29), month=10, severity=2, year=2016, start_year=2016, start_month=10, start_day=25, start_hour=12, start_minute=4, start_second=29, end_year=2016, end_month=10, end_day=25, end_hour=18, end_minute=4, end_second=29, duration_minutes=360.0, day_of_week=3, is_weekend=0, season='fall', start_lat=Decimal('40'), start_lng=Decimal('-86'), end_lat=Decimal('40'), end_lng=Decimal('-86'), street='N White River Pkwy West Dr', city='Marion', county='46222', state='IN', zipcode='US/Eastern', timezone='Indianapolis', airport_code='KEYE', temperature_f=Decimal('42'), wind_chill_f=Decimal('39'), humidity_pct=Decimal('82'), pressure_in=Decimal('30'), visibility_mi=Decimal('10'), wind_direction='East', wind_speed_mph=Decimal('5'), precipitation_in=None, weather_condition='Clear', weather_year=2016, weather_month=10, weather_day=25, weather_hour=8, weather_minute=53, weather_second=0, sunrise_sunset='Day', civil_twilight='Day', nautical_twilight='Day', astronomical_twilight='Day', amenity=False, bump=False, crossing=False, give_way=False, junction=False, no_exit=False, railway=False, roundabout=False, station=False, stop=False, traffic_calming=False, traffic_signal=False, start_alt=0.0, end_alt=0.0, start_month_encoded_sin=-0.8660254037844386, start_month_encoded_cos=0.5000000000000001, start_day_encoded_sin=-0.9377521321470804, start_day_encoded_cos=0.3473052528448203, start_hour_encoded_sin=1.2246467991473532e-16, start_hour_encoded_cos=-1.0, start_minute_encoded_sin=0.40673664307580015, start_minute_encoded_cos=0.9135454576426009, start_second_encoded_sin=0.10452846326765329, start_second_encoded_cos=-0.9945218953682734, end_month_encoded_sin=-0.8660254037844386, end_month_encoded_cos=0.5000000000000001, end_day_encoded_sin=-0.9377521321470804, end_day_encoded_cos=0.3473052528448203, end_hour_encoded_sin=-1.0, end_hour_encoded_cos=-1.8369701987210297e-16, end_minute_encoded_sin=0.40673664307580015, end_minute_encoded_cos=0.9135454576426009, end_second_encoded_sin=0.10452846326765329, end_second_encoded_cos=-0.9945218953682734, weather_month_encoded_sin=-0.8660254037844386, weather_month_encoded_cos=0.5000000000000001, weather_day_encoded_sin=-0.9377521321470804, weather_day_encoded_cos=0.3473052528448203, weather_hour_encoded_sin=0.8660254037844387, weather_hour_encoded_cos=-0.4999999999999998, weather_minute_encoded_sin=-0.6691306063588581, weather_minute_encoded_cos=0.7431448254773942, weather_second_encoded_sin=0.0, weather_second_encoded_cos=1.0, start_x=341298.02924215276, start_y=-4880789.210265539, start_z=4077985.572200376, end_x=341298.02924215276, end_y=-4880789.210265539, end_z=4077985.572200376, address_hashed=SparseVector(64, {5: 1.0, 8: 1.0, 26: 1.0, 38: 1.0, 58: 1.0}), zipcode_encoded=0.0, timezone_encoded=44.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1caefc8d-6994-4e3a-ad97-0e9b6b7c26be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Select numerical features to scale\n",
    "numerical_cols = [\n",
    "    'distance_mi', 'duration_minutes', 'temperature_f', 'wind_chill_f',\n",
    "    'humidity_pct', 'pressure_in', 'visibility_mi', 'wind_speed_mph',\n",
    "    'precipitation_in', 'start_x', 'start_y', 'start_z', 'end_x', 'end_y', 'end_z'\n",
    "]\n",
    "\n",
    "# Convert string columns to double\n",
    "for col_name in numerical_cols:\n",
    "    if col_name in df.columns and str(df.schema[col_name].dataType) == 'StringType':\n",
    "        df = df.withColumn(col_name, F.col(col_name).cast('double'))\n",
    "\n",
    "# Remove any null values (or impute)\n",
    "df = df.na.fill(0, subset=numerical_cols)\n",
    "\n",
    "# Assemble and scale features\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"numerical_features\")\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "scaler_model = pipeline.fit(df)\n",
    "df = scaler_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fd044cd-9ca4-447b-a9b4-701fdecfe3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# List of categorical columns\n",
    "# source add later\n",
    "categorical_cols = [\n",
    "     'weather_condition', 'sunrise_sunset', 'civil_twilight',\n",
    "    'nautical_twilight', 'astronomical_twilight', 'season','day_of_week'\n",
    "]\n",
    "\n",
    "# Add boolean road features\n",
    "road_bool_cols = [c for c in df.columns if c in [\n",
    "    'amenity', 'bump', 'crossing', 'give_way', 'junction', 'no_exit',\n",
    "    'railway', 'roundabout', 'station', 'stop', 'traffic_calming', 'traffic_signal', 'is_weekend'\n",
    "]]\n",
    "\n",
    "categorical_cols.extend(road_bool_cols)\n",
    "\n",
    "for col in road_bool_cols:\n",
    "    df = df.withColumn(col, F.col(col).cast(\"string\"))\n",
    "\n",
    "# String index and one-hot encode\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{c}_index\" for c in categorical_cols],\n",
    "    outputCols=[f\"{c}_encoded\" for c in categorical_cols]\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=indexers + [encoder])\n",
    "encoder_model = pipeline.fit(df)\n",
    "df = encoder_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ba9f560-2dbf-420d-8c97-3161ad29e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Get all feature columns\n",
    "feature_cols = [\n",
    "    'scaled_features',\n",
    "    *[f\"{c}_encoded\" for c in categorical_cols],\n",
    "    *[c for c in df.columns if '_encoded_sin' in c or '_encoded_cos' in c]\n",
    "]\n",
    "\n",
    "# Assemble final feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b580141e-a8e0-4bc7-8df9-9bb0c23f6451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 480:================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 87 out of 51 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VarianceThresholdSelector\n",
    "\n",
    "selector = VarianceThresholdSelector(\n",
    "    varianceThreshold=0.01,\n",
    "    featuresCol=\"features\",\n",
    "    outputCol=\"selected_features\"\n",
    ")\n",
    "\n",
    "selector_model = selector.fit(df)\n",
    "df = selector_model.transform(df)\n",
    "\n",
    "# Show which features were selected\n",
    "selected_features = selector_model.selectedFeatures\n",
    "print(f\"Selected {len(selected_features)} out of {len(feature_cols)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9284b002-a0a1-4ba8-8578-ad0edf7181ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id='A-3464290', source='Source1', distance_mi=Decimal('0.00'), start_time=datetime.datetime(2016, 10, 25, 12, 4, 29), end_time=datetime.datetime(2016, 10, 25, 18, 4, 29), month=10, severity=2, year=2016, start_year=2016, start_month=10, start_day=25, start_hour=12, start_minute=4, start_second=29, end_year=2016, end_month=10, end_day=25, end_hour=18, end_minute=4, end_second=29, duration_minutes=360.0, day_of_week=3, is_weekend='0', season='fall', start_lat=Decimal('40'), start_lng=Decimal('-86'), end_lat=Decimal('40'), end_lng=Decimal('-86'), street='N White River Pkwy West Dr', city='Marion', county='46222', state='IN', zipcode='US/Eastern', timezone='Indianapolis', airport_code='KEYE', temperature_f=Decimal('42'), wind_chill_f=Decimal('39'), humidity_pct=Decimal('82'), pressure_in=Decimal('30'), visibility_mi=Decimal('10'), wind_direction='East', wind_speed_mph=Decimal('5'), precipitation_in=Decimal('0'), weather_condition='Clear', weather_year=2016, weather_month=10, weather_day=25, weather_hour=8, weather_minute=53, weather_second=0, sunrise_sunset='Day', civil_twilight='Day', nautical_twilight='Day', astronomical_twilight='Day', amenity='false', bump='false', crossing='false', give_way='false', junction='false', no_exit='false', railway='false', roundabout='false', station='false', stop='false', traffic_calming='false', traffic_signal='false', start_alt=0.0, end_alt=0.0, start_month_encoded_sin=-0.8660254037844386, start_month_encoded_cos=0.5000000000000001, start_day_encoded_sin=-0.9377521321470804, start_day_encoded_cos=0.3473052528448203, start_hour_encoded_sin=1.2246467991473532e-16, start_hour_encoded_cos=-1.0, start_minute_encoded_sin=0.40673664307580015, start_minute_encoded_cos=0.9135454576426009, start_second_encoded_sin=0.10452846326765329, start_second_encoded_cos=-0.9945218953682734, end_month_encoded_sin=-0.8660254037844386, end_month_encoded_cos=0.5000000000000001, end_day_encoded_sin=-0.9377521321470804, end_day_encoded_cos=0.3473052528448203, end_hour_encoded_sin=-1.0, end_hour_encoded_cos=-1.8369701987210297e-16, end_minute_encoded_sin=0.40673664307580015, end_minute_encoded_cos=0.9135454576426009, end_second_encoded_sin=0.10452846326765329, end_second_encoded_cos=-0.9945218953682734, weather_month_encoded_sin=-0.8660254037844386, weather_month_encoded_cos=0.5000000000000001, weather_day_encoded_sin=-0.9377521321470804, weather_day_encoded_cos=0.3473052528448203, weather_hour_encoded_sin=0.8660254037844387, weather_hour_encoded_cos=-0.4999999999999998, weather_minute_encoded_sin=-0.6691306063588581, weather_minute_encoded_cos=0.7431448254773942, weather_second_encoded_sin=0.0, weather_second_encoded_cos=1.0, start_x=341298.02924215276, start_y=-4880789.210265539, start_z=4077985.572200376, end_x=341298.02924215276, end_y=-4880789.210265539, end_z=4077985.572200376, address_hashed=SparseVector(64, {5: 1.0, 8: 1.0, 26: 1.0, 38: 1.0, 58: 1.0}), zipcode_encoded=0.0, timezone_encoded=44.0, numerical_features=DenseVector([0.0, 360.0, 42.0, 39.0, 82.0, 30.0, 10.0, 5.0, 0.0, 341298.0292, -4880789.2103, 4077985.5722, 341298.0292, -4880789.2103, 4077985.5722]), scaled_features=DenseVector([0.0, 0.0201, 2.1149, 1.4061, 3.4743, 15.7768, 3.6333, 0.8859, 0.0, 0.2209, -10.2109, 8.5784, 0.2209, -10.2111, 8.5785]), weather_condition_index=5.0, sunrise_sunset_index=0.0, civil_twilight_index=0.0, nautical_twilight_index=0.0, astronomical_twilight_index=0.0, season_index=1.0, day_of_week_index=3.0, is_weekend_index=0.0, amenity_index=0.0, bump_index=0.0, crossing_index=0.0, give_way_index=0.0, junction_index=0.0, no_exit_index=0.0, railway_index=0.0, roundabout_index=0.0, station_index=0.0, stop_index=0.0, traffic_calming_index=0.0, traffic_signal_index=0.0, weather_condition_encoded=SparseVector(135, {5: 1.0}), sunrise_sunset_encoded=SparseVector(2, {0: 1.0}), civil_twilight_encoded=SparseVector(2, {0: 1.0}), nautical_twilight_encoded=SparseVector(2, {0: 1.0}), astronomical_twilight_encoded=SparseVector(2, {0: 1.0}), season_encoded=SparseVector(4, {1: 1.0}), day_of_week_encoded=SparseVector(7, {3: 1.0}), is_weekend_encoded=SparseVector(2, {0: 1.0}), amenity_encoded=SparseVector(2, {0: 1.0}), bump_encoded=SparseVector(2, {0: 1.0}), crossing_encoded=SparseVector(2, {0: 1.0}), give_way_encoded=SparseVector(2, {0: 1.0}), junction_encoded=SparseVector(2, {0: 1.0}), no_exit_encoded=SparseVector(2, {0: 1.0}), railway_encoded=SparseVector(2, {0: 1.0}), roundabout_encoded=SparseVector(2, {0: 1.0}), station_encoded=SparseVector(2, {0: 1.0}), stop_encoded=SparseVector(2, {0: 1.0}), traffic_calming_encoded=SparseVector(2, {0: 1.0}), traffic_signal_encoded=SparseVector(2, {0: 1.0}), features=SparseVector(225, {1: 0.0201, 2: 2.1149, 3: 1.4061, 4: 3.4743, 5: 15.7768, 6: 3.6333, 7: 0.8859, 9: 0.2209, 10: -10.2109, 11: 8.5784, 12: 0.2209, 13: -10.2111, 14: 8.5785, 20: 1.0, 150: 1.0, 152: 1.0, 154: 1.0, 156: 1.0, 159: 1.0, 165: 1.0, 169: 1.0, 171: 1.0, 173: 1.0, 175: 1.0, 177: 1.0, 179: 1.0, 181: 1.0, 183: 1.0, 185: 1.0, 187: 1.0, 189: 1.0, 191: 1.0, 193: 1.0, 195: -0.866, 196: 0.5, 197: -0.9378, 198: 0.3473, 199: 0.0, 200: -1.0, 201: 0.4067, 202: 0.9135, 203: 0.1045, 204: -0.9945, 205: -0.866, 206: 0.5, 207: -0.9378, 208: 0.3473, 209: -1.0, 210: -0.0, 211: 0.4067, 212: 0.9135, 213: 0.1045, 214: -0.9945, 215: -0.866, 216: 0.5, 217: -0.9378, 218: 0.3473, 219: 0.866, 220: -0.5, 221: -0.6691, 222: 0.7431, 224: 1.0}), selected_features=SparseVector(87, {1: 0.0201, 2: 2.1149, 3: 1.4061, 4: 3.4743, 5: 15.7768, 6: 3.6333, 7: 0.8859, 9: 0.2209, 10: -10.2109, 11: 8.5784, 12: 0.2209, 13: -10.2111, 14: 8.5785, 20: 1.0, 26: 1.0, 28: 1.0, 30: 1.0, 32: 1.0, 35: 1.0, 41: 1.0, 45: 1.0, 47: 1.0, 49: 1.0, 51: 1.0, 53: 1.0, 55: 1.0, 57: 1.0, 59: -0.866, 60: 0.5, 61: -0.9378, 62: 0.3473, 63: 0.0, 64: -1.0, 65: 0.4067, 66: 0.9135, 67: 0.1045, 68: -0.9945, 69: -0.866, 70: 0.5, 71: -0.9378, 72: 0.3473, 73: -1.0, 74: -0.0, 75: 0.4067, 76: 0.9135, 77: 0.1045, 78: -0.9945, 79: -0.866, 80: 0.5, 81: -0.9378, 82: 0.3473, 83: 0.866, 84: -0.5, 85: -0.6691, 86: 0.7431})),\n",
       " Row(id='A-3464302', source='Source1', distance_mi=Decimal('0.50'), start_time=datetime.datetime(2016, 10, 25, 12, 15, 41), end_time=datetime.datetime(2016, 10, 25, 18, 15, 41), month=10, severity=3, year=2016, start_year=2016, start_month=10, start_day=25, start_hour=12, start_minute=15, start_second=41, end_year=2016, end_month=10, end_day=25, end_hour=18, end_minute=15, end_second=41, duration_minutes=360.0, day_of_week=3, is_weekend='0', season='fall', start_lat=Decimal('40'), start_lng=Decimal('-86'), end_lat=Decimal('40'), end_lng=Decimal('-86'), street=' N Dearborn St', city='Marion', county='46218-3744', state='IN', zipcode='US/Eastern', timezone='Indianapolis', airport_code='KEYE', temperature_f=Decimal('42'), wind_chill_f=Decimal('39'), humidity_pct=Decimal('82'), pressure_in=Decimal('30'), visibility_mi=Decimal('10'), wind_direction='East', wind_speed_mph=Decimal('5'), precipitation_in=Decimal('0'), weather_condition='Clear', weather_year=2016, weather_month=10, weather_day=25, weather_hour=8, weather_minute=53, weather_second=0, sunrise_sunset='Day', civil_twilight='Day', nautical_twilight='Day', astronomical_twilight='Day', amenity='false', bump='false', crossing='true', give_way='false', junction='false', no_exit='false', railway='false', roundabout='false', station='false', stop='false', traffic_calming='false', traffic_signal='false', start_alt=0.0, end_alt=0.0, start_month_encoded_sin=-0.8660254037844386, start_month_encoded_cos=0.5000000000000001, start_day_encoded_sin=-0.9377521321470804, start_day_encoded_cos=0.3473052528448203, start_hour_encoded_sin=1.2246467991473532e-16, start_hour_encoded_cos=-1.0, start_minute_encoded_sin=1.0, start_minute_encoded_cos=2.83276944882399e-16, start_second_encoded_sin=-0.9135454576426005, start_second_encoded_cos=-0.40673664307580093, end_month_encoded_sin=-0.8660254037844386, end_month_encoded_cos=0.5000000000000001, end_day_encoded_sin=-0.9377521321470804, end_day_encoded_cos=0.3473052528448203, end_hour_encoded_sin=-1.0, end_hour_encoded_cos=-1.8369701987210297e-16, end_minute_encoded_sin=1.0, end_minute_encoded_cos=2.83276944882399e-16, end_second_encoded_sin=-0.9135454576426005, end_second_encoded_cos=-0.40673664307580093, weather_month_encoded_sin=-0.8660254037844386, weather_month_encoded_cos=0.5000000000000001, weather_day_encoded_sin=-0.9377521321470804, weather_day_encoded_cos=0.3473052528448203, weather_hour_encoded_sin=0.8660254037844387, weather_hour_encoded_cos=-0.4999999999999998, weather_minute_encoded_sin=-0.6691306063588581, weather_minute_encoded_cos=0.7431448254773942, weather_second_encoded_sin=0.0, weather_second_encoded_cos=1.0, start_x=341298.02924215276, start_y=-4880789.210265539, start_z=4077985.572200376, end_x=341298.02924215276, end_y=-4880789.210265539, end_z=4077985.572200376, address_hashed=SparseVector(64, {5: 1.0, 26: 1.0, 43: 1.0, 45: 1.0, 58: 1.0}), zipcode_encoded=0.0, timezone_encoded=44.0, numerical_features=DenseVector([0.5, 360.0, 42.0, 39.0, 82.0, 30.0, 10.0, 5.0, 0.0, 341298.0292, -4880789.2103, 4077985.5722, 341298.0292, -4880789.2103, 4077985.5722]), scaled_features=DenseVector([0.2747, 0.0201, 2.1149, 1.4061, 3.4743, 15.7768, 3.6333, 0.8859, 0.0, 0.2209, -10.2109, 8.5784, 0.2209, -10.2111, 8.5785]), weather_condition_index=5.0, sunrise_sunset_index=0.0, civil_twilight_index=0.0, nautical_twilight_index=0.0, astronomical_twilight_index=0.0, season_index=1.0, day_of_week_index=3.0, is_weekend_index=0.0, amenity_index=0.0, bump_index=0.0, crossing_index=1.0, give_way_index=0.0, junction_index=0.0, no_exit_index=0.0, railway_index=0.0, roundabout_index=0.0, station_index=0.0, stop_index=0.0, traffic_calming_index=0.0, traffic_signal_index=0.0, weather_condition_encoded=SparseVector(135, {5: 1.0}), sunrise_sunset_encoded=SparseVector(2, {0: 1.0}), civil_twilight_encoded=SparseVector(2, {0: 1.0}), nautical_twilight_encoded=SparseVector(2, {0: 1.0}), astronomical_twilight_encoded=SparseVector(2, {0: 1.0}), season_encoded=SparseVector(4, {1: 1.0}), day_of_week_encoded=SparseVector(7, {3: 1.0}), is_weekend_encoded=SparseVector(2, {0: 1.0}), amenity_encoded=SparseVector(2, {0: 1.0}), bump_encoded=SparseVector(2, {0: 1.0}), crossing_encoded=SparseVector(2, {1: 1.0}), give_way_encoded=SparseVector(2, {0: 1.0}), junction_encoded=SparseVector(2, {0: 1.0}), no_exit_encoded=SparseVector(2, {0: 1.0}), railway_encoded=SparseVector(2, {0: 1.0}), roundabout_encoded=SparseVector(2, {0: 1.0}), station_encoded=SparseVector(2, {0: 1.0}), stop_encoded=SparseVector(2, {0: 1.0}), traffic_calming_encoded=SparseVector(2, {0: 1.0}), traffic_signal_encoded=SparseVector(2, {0: 1.0}), features=SparseVector(225, {0: 0.2747, 1: 0.0201, 2: 2.1149, 3: 1.4061, 4: 3.4743, 5: 15.7768, 6: 3.6333, 7: 0.8859, 9: 0.2209, 10: -10.2109, 11: 8.5784, 12: 0.2209, 13: -10.2111, 14: 8.5785, 20: 1.0, 150: 1.0, 152: 1.0, 154: 1.0, 156: 1.0, 159: 1.0, 165: 1.0, 169: 1.0, 171: 1.0, 173: 1.0, 176: 1.0, 177: 1.0, 179: 1.0, 181: 1.0, 183: 1.0, 185: 1.0, 187: 1.0, 189: 1.0, 191: 1.0, 193: 1.0, 195: -0.866, 196: 0.5, 197: -0.9378, 198: 0.3473, 199: 0.0, 200: -1.0, 201: 1.0, 202: 0.0, 203: -0.9135, 204: -0.4067, 205: -0.866, 206: 0.5, 207: -0.9378, 208: 0.3473, 209: -1.0, 210: -0.0, 211: 1.0, 212: 0.0, 213: -0.9135, 214: -0.4067, 215: -0.866, 216: 0.5, 217: -0.9378, 218: 0.3473, 219: 0.866, 220: -0.5, 221: -0.6691, 222: 0.7431, 224: 1.0}), selected_features=SparseVector(87, {0: 0.2747, 1: 0.0201, 2: 2.1149, 3: 1.4061, 4: 3.4743, 5: 15.7768, 6: 3.6333, 7: 0.8859, 9: 0.2209, 10: -10.2109, 11: 8.5784, 12: 0.2209, 13: -10.2111, 14: 8.5785, 20: 1.0, 26: 1.0, 28: 1.0, 30: 1.0, 32: 1.0, 35: 1.0, 41: 1.0, 45: 1.0, 47: 1.0, 50: 1.0, 51: 1.0, 53: 1.0, 55: 1.0, 57: 1.0, 59: -0.866, 60: 0.5, 61: -0.9378, 62: 0.3473, 63: 0.0, 64: -1.0, 65: 1.0, 66: 0.0, 67: -0.9135, 68: -0.4067, 69: -0.866, 70: 0.5, 71: -0.9378, 72: 0.3473, 73: -1.0, 74: -0.0, 75: 1.0, 76: 0.0, 77: -0.9135, 78: -0.4067, 79: -0.866, 80: 0.5, 81: -0.9378, 82: 0.3473, 83: 0.866, 84: -0.5, 85: -0.6691, 86: 0.7431}))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87d8104d-b9ff-4bca-88c6-79ee215c8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is the DataFrame after VarianceThresholdSelector, containing:\n",
    "# 'id', 'severity', 'duration_minutes', 'selected_features', and other original columns.\n",
    "\n",
    "# --- Prepare data for CLASSIFICATION (Severity) ---\n",
    "df_cls = df.select(\n",
    "    F.col(\"id\"),\n",
    "    F.col(\"severity\").alias(\"label\"),  # 'severity' is the target\n",
    "    F.col(\"selected_features\").alias(\"features\")\n",
    ")\n",
    "# .na.drop(subset=[\"label\", \"features\"]) # Important: drop rows where label or features are null\n",
    "\n",
    "# --- Prepare data for REGRESSION (Duration) ---\n",
    "df_reg = df.select(\n",
    "    F.col(\"id\"),\n",
    "    F.col(\"duration_minutes\").alias(\"label\"), # 'duration_minutes' is the target\n",
    "    F.col(\"selected_features\").alias(\"features\")\n",
    ")\n",
    "# .na.drop(subset=[\"label\", \"features\"]) # Important: drop rows where label or features are null\n",
    "\n",
    "# Now, you will split df_classification_input for your classification models\n",
    "# And you will split df_regression_input for your regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42de2e40-c042-4e51-8650-013ea083c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  split the data into 60% training and 40% test (it is not stratified)\n",
    "(train_data_cls, test_data_cls) = df_cls.randomSplit([0.6, 0.4], seed = 10)\n",
    "(train_data_reg, test_data_reg) = df_reg.randomSplit([0.6, 0.4], seed = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f38bccaa-e120-46b5-a478-dd783d5154e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A function to run commands\n",
    "\n",
    "# save to json file\n",
    "import os\n",
    "def run(command):\n",
    "    return os.popen(command).read()\n",
    "\n",
    "def saveDF(dataframe,name):\n",
    "    dataframe.select(\"id\", \"label\", \"features\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(f\"project/data/{name}\")\n",
    "    \n",
    "    run(f\"hdfs dfs -cat project/data/{name}/*.json > ../data/{name}.json\")\n",
    "    \n",
    "saveDF(train_data_cls,'train_data_cls')\n",
    "\n",
    "saveDF(test_data_cls,'test_data_cls')\n",
    "\n",
    "saveDF(train_data_reg,'train_data_reg')\n",
    "\n",
    "saveDF(test_data_reg,'test_data_reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23439fc-0216-41a3-82f7-9f110d43219d",
   "metadata": {},
   "source": [
    "Imoprt libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea174caa-51e5-481e-9265-00829db5e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33705fcf-4d81-4e0f-a469-efc1ed7699f3",
   "metadata": {},
   "source": [
    "Helper function to save model and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfd34144-27b4-4623-bb3a-b1dca455a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_predictions(model, model_name_prefix, task_label, test_data_df, base_path=\"project\"):\n",
    "    model_full_name = f\"{model_name_prefix}_{task_label}\"\n",
    "    \n",
    "    # Save model\n",
    "    model_path_hdfs = f\"{base_path}/models/{model_full_name}\"\n",
    "    model_path_local_repo = f\"../models/{model_full_name}\" # Assuming 'models' folder in repo root\n",
    "    \n",
    "    print(f\"Saving model {model_full_name} to HDFS: {model_path_hdfs}\")\n",
    "    model.write().overwrite().save(model_path_hdfs)\n",
    "    \n",
    "    # Ensure local directory exists\n",
    "    os.makedirs(model_path_local_repo, exist_ok=True)\n",
    "    run(f\"hdfs dfs -get -f {model_path_hdfs}/* {model_path_local_repo}/\") # Copy contents\n",
    "    # For some models, -get needs the directory, not wildcard, and it copies the dir itself.\n",
    "    # If the above fails, try: run(f\"hdfs dfs -get -f {model_path_hdfs} models/\") to get the folder into local 'models/'\n",
    "    print(f\"Model {model_full_name} copied to local repository: {model_path_local_repo}\")\n",
    "\n",
    "    # Predict\n",
    "    print(f\"Making predictions with {model_full_name} on test data...\")\n",
    "    predictions_df = model.transform(test_data_df)\n",
    "\n",
    "    # Save predictions\n",
    "    predictions_path_hdfs = f\"{base_path}/output/{model_full_name}_predictions\" # Full path for HDFS save\n",
    "    predictions_path_local_csv_repo = f\"../output/{model_full_name}_predictions.csv\" # Path for final CSV in repo\n",
    "\n",
    "    print(f\"Saving predictions for {model_full_name} to HDFS: {predictions_path_hdfs}\")\n",
    "    predictions_df.select(\"label\", \"prediction\") \\\n",
    "        .coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"sep\", \",\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .save(predictions_path_hdfs) # Saves as a directory with part-files\n",
    "\n",
    "    # Ensure local output directory exists\n",
    "    os.makedirs(os.path.dirname(predictions_path_local_csv_repo), exist_ok=True)\n",
    "    run(f\"hdfs dfs -cat {predictions_path_hdfs}/part*.csv > {predictions_path_local_csv_repo}\")\n",
    "    print(f\"Predictions for {model_full_name} copied to local CSV: {predictions_path_local_csv_repo}\")\n",
    "    \n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d7a0b8-5b62-4297-82cb-747c5d2e6eee",
   "metadata": {},
   "source": [
    "TASK 1: SEVERITY PREDICTION (CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "754f7da2-cd84-40de-a7ee-a2b2bb88b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cls.cache()\n",
    "test_data_cls.cache()\n",
    "\n",
    "# Evaluators for Classification\n",
    "cls_evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "cls_evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850c2e2-ed7a-47e0-8cff-ea7f677c4eea",
   "metadata": {},
   "source": [
    " Model 1.1: Logistic Regression for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44e64ee7-521f-4347-9378-bb4bec2bef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Classification Model 1: Logistic Regression ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training Classification Model 1: Logistic Regression ---\")\n",
    "lr_cls = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "lr_cls_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr_cls.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(lr_cls.elasticNetParam, [0.0, 0.8]) \\\n",
    "    .build()\n",
    "\n",
    "lr_cls_cv = CrossValidator(estimator=lr_cls,\n",
    "                           estimatorParamMaps=lr_cls_param_grid,\n",
    "                           evaluator=cls_evaluator_f1, # Optimize for F1-score\n",
    "                           numFolds=3, # Use 2 for faster run on large data if needed\n",
    "                           parallelism=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "629e86c7-97a0-4d81-acd3-24f7d794b34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression (Classification) CV model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/19 00:17:34 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression (Classification) Hyperparameters:\n",
      "{Param(parent='LogisticRegression_f96ccf921211', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_f96ccf921211', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LogisticRegression_f96ccf921211', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_f96ccf921211', name='featuresCol', doc='features column name.'): 'features', Param(parent='LogisticRegression_f96ccf921211', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_f96ccf921211', name='labelCol', doc='label column name.'): 'label', Param(parent='LogisticRegression_f96ccf921211', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LogisticRegression_f96ccf921211', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='LogisticRegression_f96ccf921211', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_f96ccf921211', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='LogisticRegression_f96ccf921211', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_f96ccf921211', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LogisticRegression_f96ccf921211', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_f96ccf921211', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5, Param(parent='LogisticRegression_f96ccf921211', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting Logistic Regression (Classification) CV model...\")\n",
    "lr_cls_cv_model = lr_cls_cv.fit(train_data_cls)\n",
    "best_lr_cls_model = lr_cls_cv_model.bestModel\n",
    "print(\"Best Logistic Regression (Classification) Hyperparameters:\")\n",
    "print(best_lr_cls_model.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c8de25c-9917-4dc2-b509-90b8bbe7f7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model LogisticRegression_cls to HDFS: project/models/LogisticRegression_cls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression_cls copied to local repository: models/LogisticRegression_cls\n",
      "Making predictions with LogisticRegression_cls on test data...\n",
      "Saving predictions for LogisticRegression_cls to HDFS: project/output/LogisticRegression_cls_predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for LogisticRegression_cls copied to local CSV: ../output/LogisticRegression_cls_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3094:=================================================>  (190 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (Classification) - F1-Score on Test Data: 0.8717594070631641\n",
      "Logistic Regression (Classification) - Accuracy on Test Data: 0.9114721682003605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_cls_predictions = save_model_and_predictions(best_lr_cls_model, \"LogisticRegression\", \"cls\", test_data_cls)\n",
    "\n",
    "lr_cls_f1 = cls_evaluator_f1.evaluate(lr_cls_predictions)\n",
    "lr_cls_accuracy = cls_evaluator_accuracy.evaluate(lr_cls_predictions)\n",
    "print(f\"Logistic Regression (Classification) - F1-Score on Test Data: {lr_cls_f1}\")\n",
    "print(f\"Logistic Regression (Classification) - Accuracy on Test Data: {lr_cls_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc02c74-ef51-4fae-a024-eb7363a04edc",
   "metadata": {},
   "source": [
    "Model 1.2: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b1146aa-617c-459c-8b1f-5cf8a30a88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cls = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
    "\n",
    "rf_cls_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf_cls.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf_cls.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "rf_cls_cv = CrossValidator(estimator=rf_cls,\n",
    "                           estimatorParamMaps=rf_cls_param_grid,\n",
    "                           evaluator=cls_evaluator_f1, # Optimize for F1-score\n",
    "                           numFolds=3, # Use 2 for faster run\n",
    "                           parallelism=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02521613-8276-4cf2-832d-565325558b84",
   "metadata": {},
   "source": [
    "Fitting Random Forest (Classification) CV model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9c1c46b-25f1-4295-a058-b69a083a4a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/19 00:55:14 WARN DAGScheduler: Broadcasting large task binary with size 1067.2 KiB\n",
      "25/05/19 00:55:31 WARN DAGScheduler: Broadcasting large task binary with size 1496.7 KiB\n",
      "25/05/19 00:55:52 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/05/19 00:56:20 WARN DAGScheduler: Broadcasting large task binary with size 1693.1 KiB\n",
      "25/05/19 00:58:18 WARN DAGScheduler: Broadcasting large task binary with size 1076.8 KiB\n",
      "25/05/19 00:58:34 WARN DAGScheduler: Broadcasting large task binary with size 1551.6 KiB\n",
      "25/05/19 00:58:55 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/19 00:59:24 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/05/19 01:00:03 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/05/19 01:01:49 WARN DAGScheduler: Broadcasting large task binary with size 1072.7 KiB\n",
      "25/05/19 01:02:07 WARN DAGScheduler: Broadcasting large task binary with size 1506.5 KiB\n",
      "25/05/19 01:02:27 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/05/19 01:02:54 WARN DAGScheduler: Broadcasting large task binary with size 1699.5 KiB\n",
      "25/05/19 01:05:00 WARN DAGScheduler: Broadcasting large task binary with size 1075.6 KiB\n",
      "25/05/19 01:05:16 WARN DAGScheduler: Broadcasting large task binary with size 1550.0 KiB\n",
      "25/05/19 01:05:37 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/19 01:06:07 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/05/19 01:06:48 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/05/19 01:08:42 WARN DAGScheduler: Broadcasting large task binary with size 1075.2 KiB\n",
      "25/05/19 01:08:59 WARN DAGScheduler: Broadcasting large task binary with size 1517.9 KiB\n",
      "25/05/19 01:09:22 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/05/19 01:09:57 WARN DAGScheduler: Broadcasting large task binary with size 1730.3 KiB\n",
      "25/05/19 01:11:43 WARN DAGScheduler: Broadcasting large task binary with size 1078.5 KiB\n",
      "25/05/19 01:12:01 WARN DAGScheduler: Broadcasting large task binary with size 1557.5 KiB\n",
      "25/05/19 01:12:25 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/19 01:12:54 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/05/19 01:13:33 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/05/19 01:14:46 WARN DAGScheduler: Broadcasting large task binary with size 1057.2 KiB\n",
      "25/05/19 01:15:00 WARN DAGScheduler: Broadcasting large task binary with size 1490.4 KiB\n",
      "25/05/19 01:15:19 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest (Classification) Hyperparameters:\n",
      "{Param(parent='RandomForestClassifier_98dad5c0d493', name='bootstrap', doc='Whether bootstrap samples are used when building trees.'): True, Param(parent='RandomForestClassifier_98dad5c0d493', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False, Param(parent='RandomForestClassifier_98dad5c0d493', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10, Param(parent='RandomForestClassifier_98dad5c0d493', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'auto', Param(parent='RandomForestClassifier_98dad5c0d493', name='featuresCol', doc='features column name.'): 'features', Param(parent='RandomForestClassifier_98dad5c0d493', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini', Param(parent='RandomForestClassifier_98dad5c0d493', name='labelCol', doc='label column name.'): 'label', Param(parent='RandomForestClassifier_98dad5c0d493', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '', Param(parent='RandomForestClassifier_98dad5c0d493', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32, Param(parent='RandomForestClassifier_98dad5c0d493', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10, Param(parent='RandomForestClassifier_98dad5c0d493', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256, Param(parent='RandomForestClassifier_98dad5c0d493', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='RandomForestClassifier_98dad5c0d493', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='RandomForestClassifier_98dad5c0d493', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0, Param(parent='RandomForestClassifier_98dad5c0d493', name='numTrees', doc='Number of trees to train (>= 1).'): 10, Param(parent='RandomForestClassifier_98dad5c0d493', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='RandomForestClassifier_98dad5c0d493', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='RandomForestClassifier_98dad5c0d493', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='RandomForestClassifier_98dad5c0d493', name='seed', doc='random seed.'): 42, Param(parent='RandomForestClassifier_98dad5c0d493', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}\n",
      "Saving model RandomForestClassifier_cls to HDFS: project/models/RandomForestClassifier_cls\n",
      "Model RandomForestClassifier_cls copied to local repository: models/RandomForestClassifier_cls\n",
      "Making predictions with RandomForestClassifier_cls on test data...\n",
      "Saving predictions for RandomForestClassifier_cls to HDFS: project/output/RandomForestClassifier_cls_predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/19 01:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1883.4 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for RandomForestClassifier_cls copied to local CSV: ../output/RandomForestClassifier_cls_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/19 01:15:53 WARN DAGScheduler: Broadcasting large task binary with size 1681.2 KiB\n",
      "25/05/19 01:15:59 WARN DAGScheduler: Broadcasting large task binary with size 1681.2 KiB\n",
      "[Stage 3650:==================================================> (195 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Classification) - F1-Score on Test Data: 0.8785160419437835\n",
      "Random Forest (Classification) - Accuracy on Test Data: 0.9151883123043355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"...\")\n",
    "rf_cls_cv_model = rf_cls_cv.fit(train_data_cls)\n",
    "best_rf_cls_model = rf_cls_cv_model.bestModel\n",
    "print(\"Best Random Forest (Classification) Hyperparameters:\")\n",
    "print(best_rf_cls_model.extractParamMap())\n",
    "\n",
    "rf_cls_predictions = save_model_and_predictions(best_rf_cls_model, \"RandomForestClassifier\", \"cls\", test_data_cls)\n",
    "\n",
    "rf_cls_f1 = cls_evaluator_f1.evaluate(rf_cls_predictions)\n",
    "rf_cls_accuracy = cls_evaluator_accuracy.evaluate(rf_cls_predictions)\n",
    "print(f\"Random Forest (Classification) - F1-Score on Test Data: {rf_cls_f1}\")\n",
    "print(f\"Random Forest (Classification) - Accuracy on Test Data: {rf_cls_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95ef07-a227-4a27-ba76-8e60df400547",
   "metadata": {},
   "source": [
    "Compare Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "656fc510-859c-409d-9503-36fea5162955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Models Comparison:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+------------------+------------------+\n",
      "|model                                                                                                              |F1_Score          |Accuracy          |\n",
      "+-------------------------------------------------------------------------------------------------------------------+------------------+------------------+\n",
      "|LogisticRegressionModel: uid=LogisticRegression_f96ccf921211, numClasses=5, numFeatures=87                         |0.8717594070631641|0.9114721682003605|\n",
      "|RandomForestClassificationModel: uid=RandomForestClassifier_98dad5c0d493, numTrees=10, numClasses=5, numFeatures=87|0.8785160419437835|0.9151883123043355|\n",
      "+-------------------------------------------------------------------------------------------------------------------+------------------+------------------+\n",
      "\n",
      "Classification evaluation summary saved to HDFS dir: project/output/classification_evaluation and concatenated to local: ../output/classification_evaluation.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, label: smallint, features: vector]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_models_summary_data = [\n",
    "    (str(best_lr_cls_model), lr_cls_f1, lr_cls_accuracy),\n",
    "    (str(best_rf_cls_model), rf_cls_f1, rf_cls_accuracy)\n",
    "]\n",
    "cls_summary_df = spark.createDataFrame(cls_models_summary_data, [\"model\", \"F1_Score\", \"Accuracy\"])\n",
    "print(\"\\nClassification Models Comparison:\")\n",
    "cls_summary_df.show(truncate=False)\n",
    "\n",
    "cls_summary_path_hdfs = \"project/output/classification_evaluation\" # Directory for CSV parts\n",
    "cls_summary_path_local_csv = \"../output/classification_evaluation.csv\"\n",
    "cls_summary_df.coalesce(1) \\\n",
    "    .write.mode(\"overwrite\").format(\"csv\").option(\"sep\", \",\").option(\"header\", \"true\") \\\n",
    "    .save(cls_summary_path_hdfs)\n",
    "os.makedirs(os.path.dirname(cls_summary_path_local_csv), exist_ok=True)\n",
    "run(f\"hdfs dfs -cat {cls_summary_path_hdfs}/part*.csv > {cls_summary_path_local_csv}\")\n",
    "print(f\"Classification evaluation summary saved to HDFS dir: {cls_summary_path_hdfs} and concatenated to local: {cls_summary_path_local_csv}\")\n",
    "\n",
    "train_data_cls.unpersist()\n",
    "test_data_cls.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e15315-733a-4864-bff4-3ab7efc4f863",
   "metadata": {},
   "source": [
    "TASK 2: DURATION PREDICTION (REGRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0b9511f-5015-40c7-a475-b4f79eca14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluators for Regression\n",
    "reg_evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "reg_evaluator_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "138c673e-50d4-45f0-95b9-edc57a7fe940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2.1: Linear Regression for Regression\n",
    "lr_reg = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "lr_reg_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr_reg.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(lr_reg.elasticNetParam, [0.0, 0.8]) \\\n",
    "    .addGrid(lr_reg.aggregationDepth, [2, 3]) \\\n",
    "    .build()\n",
    "\n",
    "lr_reg_cv = CrossValidator(estimator=lr_reg,\n",
    "                           estimatorParamMaps=lr_reg_param_grid,\n",
    "                           evaluator=reg_evaluator_rmse, # Optimize for RMSE\n",
    "                           numFolds=3, # Use 2 for faster run\n",
    "                           parallelism=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0945203c-b461-4b7f-9cac-90e96602b060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Linear Regression (Regression) CV model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/19 01:18:51 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Linear Regression (Regression) Hyperparameters:\n",
      "{Param(parent='LinearRegression_e636edf94d7e', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 3, Param(parent='LinearRegression_e636edf94d7e', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LinearRegression_e636edf94d7e', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35, Param(parent='LinearRegression_e636edf94d7e', name='featuresCol', doc='features column name.'): 'features', Param(parent='LinearRegression_e636edf94d7e', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LinearRegression_e636edf94d7e', name='labelCol', doc='label column name.'): 'label', Param(parent='LinearRegression_e636edf94d7e', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError', Param(parent='LinearRegression_e636edf94d7e', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LinearRegression_e636edf94d7e', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='LinearRegression_e636edf94d7e', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LinearRegression_e636edf94d7e', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LinearRegression_e636edf94d7e', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto', Param(parent='LinearRegression_e636edf94d7e', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LinearRegression_e636edf94d7e', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n",
      "Saving model LinearRegression_reg to HDFS: project/models/LinearRegression_reg\n",
      "Model LinearRegression_reg copied to local repository: models/LinearRegression_reg\n",
      "Making predictions with LinearRegression_reg on test data...\n",
      "Saving predictions for LinearRegression_reg to HDFS: project/output/LinearRegression_reg_predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for LinearRegression_reg copied to local CSV: ../output/LinearRegression_reg_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4034:=================================================>      (7 + 1) / 8]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (Regression) - RMSE on Test Data: 0.009737228580546839\n",
      "Linear Regression (Regression) - R2 on Test Data: 0.9999999999996944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Fitting Linear Regression (Regression) CV model...\")\n",
    "lr_reg_cv_model = lr_reg_cv.fit(train_data_reg)\n",
    "best_lr_reg_model = lr_reg_cv_model.bestModel\n",
    "print(\"Best Linear Regression (Regression) Hyperparameters:\")\n",
    "print(best_lr_reg_model.extractParamMap())\n",
    "\n",
    "lr_reg_predictions = save_model_and_predictions(best_lr_reg_model, \"LinearRegression\", \"reg\", test_data_reg)\n",
    "\n",
    "lr_reg_rmse = reg_evaluator_rmse.evaluate(lr_reg_predictions)\n",
    "lr_reg_r2 = reg_evaluator_r2.evaluate(lr_reg_predictions)\n",
    "print(f\"Linear Regression (Regression) - RMSE on Test Data: {lr_reg_rmse}\")\n",
    "print(f\"Linear Regression (Regression) - R2 on Test Data: {lr_reg_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2118d8-782a-4f48-9b4d-8c0f62ffd520",
   "metadata": {},
   "source": [
    " Model 2.2: Gradient-Boosted Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a42548c-ae51-4df4-9d71-629701ddc945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting GBT Regressor CV model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GBT Regressor Hyperparameters:\n",
      "{Param(parent='GBTRegressor_28c91b6005f6', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False, Param(parent='GBTRegressor_28c91b6005f6', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10, Param(parent='GBTRegressor_28c91b6005f6', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'all', Param(parent='GBTRegressor_28c91b6005f6', name='featuresCol', doc='features column name.'): 'features', Param(parent='GBTRegressor_28c91b6005f6', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance', Param(parent='GBTRegressor_28c91b6005f6', name='labelCol', doc='label column name.'): 'label', Param(parent='GBTRegressor_28c91b6005f6', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '', Param(parent='GBTRegressor_28c91b6005f6', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: squared, absolute'): 'squared', Param(parent='GBTRegressor_28c91b6005f6', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32, Param(parent='GBTRegressor_28c91b6005f6', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 3, Param(parent='GBTRegressor_28c91b6005f6', name='maxIter', doc='max number of iterations (>= 0).'): 20, Param(parent='GBTRegressor_28c91b6005f6', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256, Param(parent='GBTRegressor_28c91b6005f6', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='GBTRegressor_28c91b6005f6', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='GBTRegressor_28c91b6005f6', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0, Param(parent='GBTRegressor_28c91b6005f6', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='GBTRegressor_28c91b6005f6', name='seed', doc='random seed.'): 42, Param(parent='GBTRegressor_28c91b6005f6', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='GBTRegressor_28c91b6005f6', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0, Param(parent='GBTRegressor_28c91b6005f6', name='validationTol', doc='Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol, then learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used.'): 0.01}\n",
      "Saving model GBTRegressor_reg to HDFS: project/models/GBTRegressor_reg\n",
      "Model GBTRegressor_reg copied to local repository: models/GBTRegressor_reg\n",
      "Making predictions with GBTRegressor_reg on test data...\n",
      "Saving predictions for GBTRegressor_reg to HDFS: project/output/GBTRegressor_reg_predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for GBTRegressor_reg copied to local CSV: ../output/GBTRegressor_reg_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "gbt_reg = GBTRegressor(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
    "\n",
    "gbt_reg_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt_reg.maxIter, [10, 20]) \\\n",
    "    .addGrid(gbt_reg.maxDepth, [3, 5]) \\\n",
    "    .build()\n",
    "\n",
    "gbt_reg_cv = CrossValidator(estimator=gbt_reg,\n",
    "                            estimatorParamMaps=gbt_reg_param_grid,\n",
    "                            evaluator=reg_evaluator_rmse, # Optimize for RMSE\n",
    "                            numFolds=3, # Use 2 for faster run\n",
    "                            parallelism=2)\n",
    "\n",
    "print(\"Fitting GBT Regressor CV model...\")\n",
    "gbt_reg_cv_model = gbt_reg_cv.fit(train_data_reg)\n",
    "best_gbt_reg_model = gbt_reg_cv_model.bestModel\n",
    "print(\"Best GBT Regressor Hyperparameters:\")\n",
    "print(best_gbt_reg_model.extractParamMap())\n",
    "\n",
    "gbt_reg_predictions = save_model_and_predictions(best_gbt_reg_model, \"GBTRegressor\", \"reg\", test_data_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3173653-8d5f-4e94-983f-21c9d785df00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6867:=================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT Regressor - RMSE on Test Data: 11004.026788553107\n",
      "GBT Regressor - R2 on Test Data: 0.6095892615385172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "gbt_reg_rmse = reg_evaluator_rmse.evaluate(gbt_reg_predictions)\n",
    "gbt_reg_r2 = reg_evaluator_r2.evaluate(gbt_reg_predictions)\n",
    "print(f\"GBT Regressor - RMSE on Test Data: {gbt_reg_rmse}\")\n",
    "print(f\"GBT Regressor - R2 on Test Data: {gbt_reg_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ec3640-a7d9-41be-8ea0-0b034fd066ef",
   "metadata": {},
   "source": [
    "Compare Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "642379d7-d41f-4e1d-8268-8cac60edf16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression Models Comparison:\n",
      "+------------------------------------------------------------------------------+--------------------+------------------+\n",
      "|model                                                                         |RMSE                |R2                |\n",
      "+------------------------------------------------------------------------------+--------------------+------------------+\n",
      "|LinearRegressionModel: uid=LinearRegression_e636edf94d7e, numFeatures=87      |0.009737228580546839|0.9999999999996944|\n",
      "|GBTRegressionModel: uid=GBTRegressor_28c91b6005f6, numTrees=20, numFeatures=87|11004.026788553107  |0.6095892615385172|\n",
      "+------------------------------------------------------------------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reg_models_summary_data = [\n",
    "    (str(best_lr_reg_model), lr_reg_rmse, lr_reg_r2),\n",
    "    (str(best_gbt_reg_model), gbt_reg_rmse, gbt_reg_r2)\n",
    "]\n",
    "reg_summary_df = spark.createDataFrame(reg_models_summary_data, [\"model\", \"RMSE\", \"R2\"])\n",
    "print(\"\\nRegression Models Comparison:\")\n",
    "reg_summary_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0dbf2b77-96c9-40d7-ad68-06988a5858df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression evaluation summary saved to HDFS dir: project/output/regression_evaluation and concatenated to local: output/regression_evaluation.csv\n",
      "\n",
      "--- All tasks completed. Stopping Spark session. ---\n"
     ]
    }
   ],
   "source": [
    "reg_summary_path_hdfs = \"project/output/regression_evaluation\" # Directory for CSV parts\n",
    "reg_summary_path_local_csv = \"../output/regression_evaluation.csv\"\n",
    "reg_summary_df.coalesce(1) \\\n",
    "    .write.mode(\"overwrite\").format(\"csv\").option(\"sep\", \",\").option(\"header\", \"true\") \\\n",
    "    .save(reg_summary_path_hdfs)\n",
    "os.makedirs(os.path.dirname(reg_summary_path_local_csv), exist_ok=True)\n",
    "run(f\"hdfs dfs -cat {reg_summary_path_hdfs}/part*.csv > {reg_summary_path_local_csv}\")\n",
    "print(f\"Regression evaluation summary saved to HDFS dir: {reg_summary_path_hdfs} and concatenated to local: {reg_summary_path_local_csv}\")\n",
    "\n",
    "train_data_reg.unpersist()\n",
    "test_data_reg.unpersist()\n",
    "\n",
    "print(\"\\n--- All tasks completed. Stopping Spark session. ---\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
